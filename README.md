# Qwen Finetuning on (MATLAB) Dataset

This repository explores the internal reasoning and representational capacity of Qwen LLMs when fine-tuned on a small, domain-specific dataset containing MATLAB code. The goal is to understand how Qwen models interpret structured technical data, with a focus on explainability, reliability, and conceptual grounding.

---

## Dataset

- **Dataset**:  
  MATLAB code snippets paired with human-interpretable plot descriptions.  
  ðŸ‘‰ [View on Hugging Face](https://huggingface.co/datasets/philip120/RPOFES-dataset)

---

## Experiments

| Task                        | Notebook Link                                                                                   | Description                                           |
|-----------------------------|--------------------------------------------------------------------------------------------------|-------------------------------------------------------|
| Tokenizer Test           | [Colab](https://colab.research.google.com/drive/1rERnFmT16HUkwmwDt5YKpgPL5uv0CQXJ?usp=sharing)   | Tests Qwen tokenizer behavior on MATLAB code         |
| Oracle Comparison        | [Colab](https://colab.research.google.com/drive/1IEZKTnfRhPhBYzpSe1kEiBFdmygi1uwB?usp=sharing)   | Baseline performance comparison with pretrained Qwen |
| ðŸ› Finetuning with Unsloth | [Colab](https://colab.research.google.com/drive/1WHo2yAzJO9B21UsaKfGjZV2ZrRyhvKlQ?usp=sharing)   | Lightweight finetuning using Unsloth + LoRA          |

---

## Motivation

This work is part of a broader research initiative aimed at:

- Enhancing **LLM interpretability** for niche and multimodal datasets  
- Investigating **model safety** via internal representation analysis  
- Developing **task-aligned compact models** through distillation and fine-tuning  

It draws from current research on trustworthy AI, multimodal reasoning, and conceptual mapping to improve model transparency and usability in sensitive or high-reliability domains.

---

## Related Work

Part of a larger project on **trustworthy and safe LLMs through multimodal understanding** â€” focusing on controlled experiments with synthetic, interpretable data to map model knowledge.

